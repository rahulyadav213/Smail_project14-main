{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee692818",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Infra Files ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37435ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ----- CONFIG -----\n",
    "SEQ_LEN = 25\n",
    "IMG_SIZE = 224\n",
    "DROP_P = 0.3\n",
    "NUM_CLASSES = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CLASS_NAMES = [\n",
    "    'cover', 'defense', 'flick', 'hook', 'late_cut', \n",
    "    'lofted', 'pull', 'square_cut', 'straight', 'sweep'\n",
    "]\n",
    "\n",
    "# ----- Extract ground truth from filename -----\n",
    "video_path = \"cover_0001.avi\"\n",
    "gt_label_name = Path(video_path).stem.split(\"_\")[0]  # e.g., 'cover'\n",
    "gt_label_text = f\"GT: {gt_label_name}\"\n",
    "\n",
    "# ----- Model Definitions -----\n",
    "def get_backbone():\n",
    "    m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    m.fc = nn.Identity()\n",
    "    return m\n",
    "\n",
    "class CNN_RNN(nn.Module):\n",
    "    def __init__(self, backbone, rnn_type=\"LSTM\", bidir=False, drop_p=DROP_P):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        hidden = 256\n",
    "        rnn_cls = {\"LSTM\": nn.LSTM, \"GRU\": nn.GRU}[rnn_type]\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=512, \n",
    "            hidden_size=hidden,\n",
    "            batch_first=True, \n",
    "            bidirectional=bidir\n",
    "        )\n",
    "        mult = 2 if bidir else 1\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.head = nn.Linear(hidden * mult, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C, H, W)\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        feats = self.backbone(x)          # (B*T, feat_dim)\n",
    "        feats = feats.view(B, T, -1)      # (B, T, feat_dim)\n",
    "        out, _ = self.rnn(feats)          # (B, T, hidden*mult)\n",
    "        x_last = out[:, -1, :]            # (B, hidden*mult)\n",
    "        x_drop = self.dropout(x_last)\n",
    "        return self.head(x_drop)          # (B, NUM_CLASSES)\n",
    "\n",
    "# ----- Load Models -----\n",
    "yolo_model = YOLO(\"best.pt\")\n",
    "cnn_lstm_model = CNN_RNN(get_backbone(), \"LSTM\", False).to(DEVICE)\n",
    "cnn_lstm_model.load_state_dict(\n",
    "    torch.load(\"CNN-LSTM_best.pth\", map_location=DEVICE)\n",
    ")\n",
    "cnn_lstm_model.eval()\n",
    "\n",
    "# ----- Transform -----\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406], \n",
    "        [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# ----- Prepare Video I/O -----\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "out_vid = cv2.VideoWriter(\n",
    "    \"output_with_preds1.mp4\",\n",
    "    cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "    fps, \n",
    "    (w, h)\n",
    ")\n",
    "\n",
    "# ----- Buffers & State -----\n",
    "roi_seq = []\n",
    "frame_buffer = []\n",
    "last_pred_label_text = \"Pred: ---\"  # initialize with placeholder\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Store for later annotation\n",
    "    frame_buffer.append(frame.copy())\n",
    "\n",
    "    # Run YOLO detection on the current frame\n",
    "    results = yolo_model(frame, verbose=False)[0]\n",
    "    bboxes = results.boxes.xyxy.cpu().numpy()\n",
    "\n",
    "    if len(bboxes) > 0:\n",
    "        x1, y1, x2, y2 = map(int, bboxes[0])  # first detected ROI\n",
    "        roi = frame[y1:y2, x1:x2]\n",
    "        roi_pil = Image.fromarray(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))\n",
    "        roi_tensor = transform(roi_pil)\n",
    "        roi_seq.append(roi_tensor)\n",
    "\n",
    "    # When we have SEQ_LEN ROIs, run the sequence classifier\n",
    "    if len(roi_seq) == SEQ_LEN:\n",
    "        with torch.no_grad():\n",
    "            input_seq = torch.stack(roi_seq).unsqueeze(0).to(DEVICE)\n",
    "            logits = cnn_lstm_model(input_seq)\n",
    "            pred_class = logits.argmax(1).item()\n",
    "            # Update the last prediction\n",
    "            last_pred_label_text = f\"Pred: {CLASS_NAMES[pred_class]}\"\n",
    "\n",
    "        # Annotate all buffered frames with GT and this prediction\n",
    "        for f in frame_buffer:\n",
    "            cv2.putText(\n",
    "                f, gt_label_text, (30, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2,\n",
    "                (255, 0, 0), 3\n",
    "            )\n",
    "            cv2.putText(\n",
    "                f, last_pred_label_text, (30, 100),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2,\n",
    "                (0, 255, 0), 3\n",
    "            )\n",
    "            out_vid.write(f)\n",
    "\n",
    "        # Reset buffers for next sequence\n",
    "        roi_seq.clear()\n",
    "        frame_buffer.clear()\n",
    "\n",
    "# After video ends, write any leftover frames using the last prediction\n",
    "for f in frame_buffer:\n",
    "    cv2.putText(\n",
    "        f, gt_label_text, (30, 50),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 1.2,\n",
    "        (255, 0, 0), 3\n",
    "    )\n",
    "    cv2.putText(\n",
    "        f, last_pred_label_text, (30, 100),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 1.2,\n",
    "        (0, 255, 0), 3\n",
    "    )\n",
    "    out_vid.write(f)\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "out_vid.release()\n",
    "print(\"âœ… Output video saved as 'output_with_preds.mp4'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
